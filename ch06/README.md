# 第6章 学習に関するテクニック
## 最適化(optimaization)
損失関数の値をできるだけ小さくするパラメータを見つけることを **最適化(optimaization)** という。  
その一つの手法として **確率的勾配降下法(Stochastic gradient descent - SGD)** がある。  

## 最適化の種類
### SGD（確率的勾配下降法）
勾配方向へ一定の距離だけ進む方法。  
単純で実装も簡単だが、船艇型のような関数に適用する場合、ジグザグな動きをして非効率な経路を探索するという欠点がある。  

### Momentum
ボールが地面を転がるような動きで探索する。  
SGDに比べてジグザグの動きが軽減される。「Momentum」は運動量を表す言葉。

### AdaGrad
パラメータの要素ごとに適応的に学習計数を調整しながら学習を行う手法。  
パラメータ更新の際に学習のスケールを調整し、パラメータの要素のなかでよく動いた（大きく更新された）要素は学習計数が小さくなるよう調整する。（学習計数の減衰）

### Adam
MomentumとAdaGradのアイディアを融合した探索方法。  
またハイパーパラメータの「バイアス補正（偏りの補正）」が行われることも特徴の一つ。

##重みの初期値
重みの初期値を適切に設定することで、各層のアクティベーションの分布に適度な広がりをもたせ、学習をスムーズに進める。  


### 荷重減衰(Weaight Decay)
重みパラメータの値が小さくなるように学習を行うことを目的とした手法。重みの値を小さくすることで過学習が起きにくくなる。  
しかし初期値を0にすると誤差逆伝搬法において全ての重みの値が均一に(同じように)更新されてしまい、学習が進まない。

###Xavierの初期値
前層のノードの個数をnとした場合、1/√nの標準偏差を持つ分布を使うことで、各層のアクティベーション（活性化関数の後の出力データ）を同じ広がりのある分布にすることができる。  
活性化関数が線形であることが前提。sigmoidやtanhなどS字カーブの活性化関数に適している。（左右対称で、中央付近が線形関数としてみなせるため）

###Heの初期値
活性化関数にReLUを用いる場合に特化した初期値。前層のノードがn個の場合、√(2/n)を標準偏差とするガウス分布を用いる。

### 比較グラフ
![比較グラフ](https://github.com/nesheep5/study-deep-learning-from-scratch/blob/master/ch06/Figure_1.png)

## Batch Normalization
学習をスムーズに行うため、各層で適度は広がりを持つように"強制的"にアクティベーションの分布を調整する手法。  
次のような利点がある。
- 学習を速く進行させることが出来る(学習係数を大きくすることができる)
- 初期値にそれほど依存しない(初期値に対してそこまで神経質にならなくてよい)
- 過学習を抑制する(Dropoutなどの必要性をなくす)

具体的には、データの分布が平均が0で分散が1になるように正規化をおこなう。

## 正則化
### 過学習
訓練データにだけ対応してしまい、訓練データに含まれない他のデータにはうまく対応できない状態。  
過学習が起きる原因として、次の２つが挙げられる
- パラメータを大量に持ち、表現力の高いモデルであること
- 訓練データが少ないこと

### weight decay(荷重減衰)
過学習抑制に昔から用いられる手法。  
学習の過程において、大きな重みを持つことに対してペナルティを課すことで過学習を抑制する。  
(重みパラメータが大きな値をとることにより、過学習が発生することが多いため)

