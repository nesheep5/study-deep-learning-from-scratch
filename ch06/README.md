# 第6章 学習に関するテクニック
## 最適化(optimaization)
損失関数の値をできるだけ小さくするパラメータを見つけることを **最適化(optimaization)** という。  
その一つの手法として **確率的勾配降下法(Stochastic gradient descent - SGD)** がある。  

## 最適化の種類
### SGD（確率的勾配下降法）
勾配方向へ一定の距離だけ進む方法。  
単純で実装も簡単だが、船艇型のような関数に適用する場合、ジグザグな動きをして非効率な経路を探索するという欠点がある。  
### Momentum

### AdaGrad

### Adam
