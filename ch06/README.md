# 第6章 学習に関するテクニック
## 最適化(optimaization)
損失関数の値をできるだけ小さくするパラメータを見つけることを **最適化(optimaization)** という。  
その一つの手法として **確率的勾配降下法(Stochastic gradient descent - SGD)** がある。  

## 最適化の種類
### SGD（確率的勾配下降法）
勾配方向へ一定の距離だけ進む方法。  
単純で実装も簡単だが、船艇型のような関数に適用する場合、ジグザグな動きをして非効率な経路を探索するという欠点がある。  

### Momentum
ボールが地面を転がるような動きで探索する。  
SGDに比べてジグザグの動きが軽減される。「Momentum」は運動量を表す言葉。

### AdaGrad
パラメータの要素ごとに適応的に学習計数を調整しながら学習を行う手法。  
パラメータ更新の際に学習のスケールを調整し、パラメータの要素のなかでよく動いた（大きく更新された）要素は学習計数が小さくなるよう調整する。（学習計数の減衰）

### Adam
MomentumとAdaGradのアイディアを融合した探索方法。  
またハイパーパラメータの「バイアス補正（偏りの補正）」が行われることも特徴の一つ。

